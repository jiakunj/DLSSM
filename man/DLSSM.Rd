% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/DLSSM.R
\name{DLSSM}
\alias{DLSSM}
\title{Dynamic logistic state-space prediction model for binary outcomes}
\usage{
DLSSM(fit0, data.batched, K)
}
\arguments{
\item{fit0}{initial fitted model with training data}

\item{data.batched}{batched dataset generated by function Batched()}

\item{K}{Number of steps for ahead prediction}
}
\value{
\tabular{ll}{
   \code{pred.K:} \tab K-steps ahead predicted coefficients \cr
   \tab \cr
   \code{pred.var.K:} \tab  covariance of K-steps ahead predicted coefficients \cr
   \tab \cr
   \code{pred.prob.K:} \tab  K-steps ahead predicted probabilities \cr
 }
}
\description{
Implements dynamic logistic state-space prediction model for binary outcomes as described in Jiakun et al.(2021, Biometrics). In retrospective study,
it is suitable to use the main function DLSSM with specify trainning sample size. The results can also be repeated using subfunctions which is more suitable for online implementation. The algorithm was composed by training part and validation part.
On the stage of training, the smoothing parameters were selected by maximizing likelihood function. Then, based on the estimated smoothing parameters, run the Kalman filtering and
smoothing algorithm to estimate both the time-varying and time-invariant coefficients in the model. Based on the estimated coefficients and state-sapce model, it was straightforward to do prediction.
}
\details{
The argument fit could be object of DLSSM or DLSSM.init.
}
\examples{
rm(list=ls())
set.seed(12)
n=8000
beta0=function(t)   0.1*t-1
beta1=function(t)  cos(2*pi*t)
beta2=function(t)  sin(2*pi*t)
alph1=alph2=1
x=matrix(runif(n*4,min=-4,max=4),nrow=n,ncol=4)
t=sort(runif(n))
coef=cbind(beta0(t),beta1(t),beta2(t),rep(alph1,n),rep(alph2,n))
covar=cbind(rep(1,n),x)
linear=apply(coef*covar,1,sum)
prob=exp(linear)/(1+exp(linear))
y=as.numeric(runif(n)<prob)
sim.data=cbind(y,x,t)
colnames(sim.data)=c("y","x1","x2","x3","x4","t")
formula = y~x1+x2+x3+x4
# Divide the time domain [0,1] into S=100 equally spaced intervals and then generated S=100 batches of data
S=100
S0=75
data.batched=Batched(formula, data=sim.data, time="t", S)

# using first 75 batches as training dataset to tune smoothing parameters
fit0=DLSSM.init(data.batched, S0, vary.effects=c("x1","x2"))
fit0$Lambda


# After initial model fitting on training dataset, we move to dynamic prediction
# Following is  using DLSSM in a "streaming" mode which recursively apply prediction and filtering.
 K=1
 dimens=2*3+2   # length of state vector
 num.valid=S-S0+(K-1)  # number of predicted batches
 Prediction=matrix(NA,num.valid,dimens)  # predicted state vector
 Prediction.var=array(NA,dim=c(num.valid,dimens,dimens)) # covariance
 Filtering=matrix(NA,num.valid,dimens)  # filtering of state vector
 Filtering.var=array(NA,dim=c(num.valid,dimens,dimens))  # covariance
 Pred.prob=list()   # predicted probabilities
 fit.last=fit0
 for(i in 1:25){
   new.batch=data.batched$batched[[i+75]]
   newx=new.batch[,c("x1","x2","x3","x4")]
   pred=DLSSM.predict(fit.last,K=1,newx=newx)
   Prediction[i,]=pred$coef.pred
   Prediction.var[i,,]=pred$coef.pred.var
   Pred.prob[[i]]=pred$prob.pred
   fit.last=DLSSM.filter(fit.last,data.batched$batched[[i+75]])
   Filtering[i,]=fit.last$Filter
   Filtering.var[i,,]=fit.last$Filter.var
 }

 # The above iterative procedure can be easily replaced by  function DLSSM
 fit=DLSSM(fit0, data.batched, K=1)
 DLSSM.plot(fit)
}
\author{
Jiakun Jiang, Wei Yang, Stephen E. Kimmel and Wensheng Guo
}
